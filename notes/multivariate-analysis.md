# 多変量解析

<!-- HACK: 項目の並び順 -->
2つ以上の変数が相互に関連している場合に、その関連性を分析するための統計的手法です

データ間の関連性を捉えることで予測や要因分析行える。

# 多変量解析でわかる関係性
- 相関関係
- 因果関係

# 解析手法(今回扱う)
- 単変量解析
    - 相関分析：変数同士の相関を見る分析手法。影響度合いを数値化。
        - 散布図
        - ヒートマップ
    - 単回帰分析：
- 多変量解析
    - 重回帰分析

# 相関係数の特徴
- 相関係数は、-1から1の間の値をとる
- 相関係数は、絶対値が大きいほど、相関が強い
- 相関係数は
    - 正の場合：正の相関
    - 負の場合：負の相関

# 疑似相関
2つの変数間に統計的な相関が観察されるが、その相関が実際には偶然または第三の要因によるものであるという誤った結論を導くこと

# 回帰係数 vs 偏回帰係数
- 回帰係数：説明変数と目的変数の関係を表す線形モデル（単回帰分析または重回帰分析）において、説明変数と目的変数の間の単位の変化に対して、目的変数がどれだけ変化するかを表す数値。
説明変数と目的変数の間の関係を表す線の傾きを表す。
- 偏回帰係数：複数の説明変数がある多重共線性のある回帰分析において、1つの説明変数を他の説明変数が一定であると仮定したときの目的変数との関係を表す数値です。すなわち、1つの説明変数が他の説明変数の影響を受けずに、目的変数にどの程度影響を与えるかを表します。偏回帰係数は、多重共線性を考慮して回帰係数を求めるために使用する。

簡単にまとめると、回帰係数は単回帰分析や重回帰分析で説明変数と目的変数の関係を表す係数であり、偏回帰係数は多重共線性を考慮した回帰係数の一種であり、1つの説明変数が他の説明変数の影響を受けずに目的変数にどの程度影響を与えるかを表す係数

# 回帰分析を使用した検定


- ## ゼロ重み検定 (zero weight test) 
ある説明変数の重みが0であることを仮定した上で、その仮定が正しいかどうかを検定する方法。

主に変数選択のために用いられます。たとえば、複数の説明変数がある場合、モデルに含めるべき説明変数を選択する際に、ある説明変数が目的変数に対して影響を与えていないと判断された場合、その説明変数をモデルから削除することができます。


- ## ウォルド検定 (Wald test) 
説明変数の重みだけでなく、回帰係数の標準誤差も考慮に入れた検定を行うことができます

自由度調整済み決定係数 (adjusted R-squared) の改善や、回帰係数の標準誤差の算出などにも利用されます。

- ## ゼロ重み検定とウォルド検定の関係
本質的には同じものであると言えます。つまり、ゼロ重み検定はウォルド検定の特殊な場合であり、ウォルド検定はゼロ重み検定をより一般化したものと言えます。

# p値とt値
回帰分析で使用されるt値とp値は、統計的に有意な係数を特定するために使用
- t値： 係数 / 標準誤差
- p値： t値と同様に、各説明変数の係数がゼロであるかどうかを判断するために使用されます。t分布の自由度とt値を使用して、説明変数の係数がゼロである確率（つまり、帰無仮説が真である確率）を計算した値です。

# カテゴリ変数の取り扱い
- Label Encoding：カテゴリ変数を数値に変換する手法。カテゴリ変数に対して、0から始まる整数値を割り当てることで数値化します。
- One Hot Encoding：カテゴリ変数をダミー変数化する手法。、カテゴリ変数の各カテゴリを列として表し、そのカテゴリに該当する場合は1、該当しない場合は0を割り当てる方法。
※ダミー変数：0と1で表される変数

# 特徴量
分析や予測の対象となるデータの属性や特性を表す数値、カテゴリ、テキストなどの値のことを指します 

# 主成分分析（PCA: Principal Component Analysis）
多変量データの解析手法の1つで、データをより少ない数の要素（主成分）に要約する方法。

可能な限り元のデータの情報を保持しつつ、データの次元を削減するべき。

# 主成分分析の手順（ネット調べ）
1. データの中心化：各変数の平均を0に揃えます。
1. 共分散行列の算出：データの分散・共分散行列を求めます。
1. 固有値分解：共分散行列を固有ベクトルと固有値に分解します。
1. 主成分の決定：固有値が大きい順に固有ベクトルを取り出して、主成分を決定します。
1. 主成分得点の算出：主成分を利用して、元のデータの主成分得点を算出します。

# 主成分分析の手順（コースで扱ったもの）
1. 分散が最大となる軸（第1主成分）を決定
1. 軸に元のデータを射影
1. 第1主成分と直行するような軸（第2主成分）を決定

# 主成分分析の注意点
- データを削減するので、元のデータの情報を失うことがあります。
- 各主成分の意味は、元のデータの変数の重みの組み合わせになるので、人間側で解釈する必要がある。

# 主成分分析の活用方法
- データを可視化したいケース<br>
    人間は3次元までしか視覚的に理解できないため、データを可視化するためには、次元を削減する必要があるから。
- データ量を削減したいケース<br>
    計算のコストの観点から、下げた方が良い場合がある。<br>
    主成分で、元のデータをしっかり表現できているなら、コストが低い方が良い。

# 主成分分析の評価方法
- 寄与率：各主成分の分散の割合。次元削減後のデータがどれだけ元のデータを表現できているかを表す指標。
- 累積寄与率：各主成分の分散の割合の累積。上記の寄与率の累積。
- 主成分負荷量：各主成分の元の変数の重み。各元変数と主成分との間の相関係数を示す指標。見方は、プラスの場合は正の相関で値が高いほど主成分と正の相関が強い、マイナスの場合は負の相関で、値が低いほど主成分と負の相関が強い。

※主成分負荷量を求めるにあたって、固有ベクトルと固有値を求める必要があるが、固有ベクトルは、主成分の方向を表すベクトル（分散が最大になる方向を表す）で、固有値は、主成分の分散を表す値である（分散の大きさを表す）。

# 主成分得点 
主成分分析において、各観測値（個体やサンプル）が、得られた主成分に対してどの程度寄与しているかを示す指標です。

主成分分析によって得られた主成分と元の変数との間の線形結合で表されます。つまり、各観測値の元の変数の値を、主成分負荷量と主成分の値を用いて計算して得られる値です。

主成分分析の結果を解釈する際に重要な役割を果たします。主成分得点は、各観測値が得られた主成分の値にどの程度寄与しているかを示すため、観測値のグルーピングやクラスタリング、分類などの解析に利用されます。また、主成分得点を用いたグラフ表示や図示方法もあり、主成分分析の結果を視覚的に理解することができます

# クラスタリング
類似性からグループ分けできる。そして、人間のバイアスを排除して、グループ分けできる。

与えられたデータセットを似たもの同士でグループ分けする手法。

与えられたデータセット内のデータポイントの相似度に基づいて、クラスタと呼ばれるグループを作成します。データポイントは、通常、ベクトルとして表現され、ベクトル間の距離が相似度の尺度として使用されます。クラスタリングの目的は、同じクラスタ内のデータポイントができるだけ似たもの同士であり、異なるクラスタのデータポイントは異なるもの同士であるようにすることです

# 階層的 vs 非階層的
- 階層的：階層的な木構造を作成し、最も似たもの同士のクラスタが最上位にあり、最も異なるもの同士のクラスタが最下位にあるようにする。サンプルサイズが小さい時に使いがち
- 非階層的：あらかじめ指定されたクラスタ数に基づいてクラスタを作成する。サンプルサイズが大きい場合はこっちを使うことが多い。

# クラスタリングの注意点
- クラスタリングの結果は、初期値に依存する。（非階層的クラスタリングの場合）
- 機械的に類似するものをグループ分けしただけなので、その分けられたモノが意味を持つかどうかは、人間が判断する必要がある。

# クラスタリングのアルゴリズム
- k-means法<br>
    最もよく知られたクラスタリングアルゴリズムの一つであり、データセットをK個のクラスタに分割します。K-meansアルゴリズムでは、各クラスタについて重心を計算し、各データポイントをその重心に最も近いクラスタに割り当てます。
- 階層的クラスタリング法<br>
    データポイントを階層的な木構造として表現するアルゴリズムであり、クラスタを一つずつマージすることで木構造を作成します。階層的クラスタリングには、凝集型クラスタリングと分割型クラスタリングがあります。
- DBSCAN法<br>
    密度連結性に基づいて、クラスタを検出するアルゴリズムです。データセット内のデータポイントの密度に基づいて、クラスタを作成します。データポイントの密度が高い領域は、クラスタとして認識されます。
- ガウス混合モデル<br>
    多数のガウス分布を混合させたモデルを使用して、クラスタを表現するアルゴリズムです。ガウス混合モデルは、データポイントが混合された多数のガウス分布から生成されたものであると仮定して、各データポイントのクラスタを決定します。

※今回講義で扱うのは、k-means法。